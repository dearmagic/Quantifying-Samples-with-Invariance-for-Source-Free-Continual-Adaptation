begin
Traceback (most recent call last):
  File "/public/home/imgbreaker/Desktop/CISFDA/CISFDA/main_test.py", line 159, in <module>
    pretrained_net.load_state_dict(torch.load(args.source_model))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/public/home/imgbreaker/anaconda3/envs/CIUDA/lib/python3.11/site-packages/torch/serialization.py", line 1026, in load
    return _load(opened_zipfile,
           ^^^^^^^^^^^^^^^^^^^^^
  File "/public/home/imgbreaker/anaconda3/envs/CIUDA/lib/python3.11/site-packages/torch/serialization.py", line 1438, in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
  File "/public/home/imgbreaker/anaconda3/envs/CIUDA/lib/python3.11/site-packages/torch/serialization.py", line 1408, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/public/home/imgbreaker/anaconda3/envs/CIUDA/lib/python3.11/site-packages/torch/serialization.py", line 1382, in load_tensor
    wrap_storage=restore_location(storage, location),
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/public/home/imgbreaker/anaconda3/envs/CIUDA/lib/python3.11/site-packages/torch/serialization.py", line 391, in default_restore_location
    result = fn(storage, location)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/public/home/imgbreaker/anaconda3/envs/CIUDA/lib/python3.11/site-packages/torch/serialization.py", line 271, in _cuda_deserialize
    return obj.cuda(device)
           ^^^^^^^^^^^^^^^^
  File "/public/home/imgbreaker/anaconda3/envs/CIUDA/lib/python3.11/site-packages/torch/_utils.py", line 115, in _cuda
    untyped_storage = torch.UntypedStorage(
                      ^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.33 GiB of which 13.81 MiB is free. Process 1972910 has 32.89 GiB memory in use. Process 13909 has 2.76 GiB memory in use. Process 2553159 has 1.37 GiB memory in use. Process 2596639 has 2.31 GiB memory in use. Process 2331504 has 7.77 GiB memory in use. Process 2332108 has 7.97 GiB memory in use. Process 2333354 has 7.86 GiB memory in use. Process 3513235 has 6.13 GiB memory in use. Process 3513925 has 6.22 GiB memory in use. Process 4153218 has 1.16 GiB memory in use. Process 4153230 has 1.57 GiB memory in use. Including non-PyTorch memory, this process has 472.00 MiB memory in use. Process 4173512 has 416.00 MiB memory in use. Process 4173513 has 416.00 MiB memory in use. Of the allocated memory 47.78 MiB is allocated by PyTorch, and 10.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR conda.cli.main_run:execute(124): `conda run python -u /public/home/imgbreaker/Desktop/CISFDA/CISFDA/main_test.py --source_model /public/home/imgbreaker/CIUDA/ProCA-main/ProCA-main/model_source/office_home/20240414-0239-OH_Product_ce_singe_gpu_resnet50_best_param.pth --weight_model /public/home/imgbreaker/CIUDA/ProCA-main/ProCA-main/model_source/office_home/negative/20240415-0055-OH_Product_ce_singe_gpu_resnet50_best_neg_param.pth --source Product --target Art --model_name final_officehome --txt Product2Art.txt --dataset office-home/Art --num_class 65` failed. (See above for error)
begin
start training
data loaded
momentum_net loaded
moco_model loaded
optimizer loaded
weight_dict got
[1, 2, 3, 4, 5, 6, 8, 9, 34]
tensor([15.9408, 42.3954, 17.8995, 23.8179, 17.8436, 15.9334, 17.9830, 16.7760,
        18.7534], device='cuda:0')
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
data loaded
momentum_net loaded
moco_model loaded
optimizer loaded
weight_dict got
[10, 13, 15, 19]
tensor([16.4007, 32.4031, 23.3241, 49.0470], device='cuda:0')
[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
data loaded
momentum_net loaded
moco_model loaded
optimizer loaded
weight_dict got
[21, 23, 24, 25]
tensor([37.4185, 15.8034, 15.2310, 28.3005], device='cuda:0')
[20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
data loaded
momentum_net loaded
moco_model loaded
optimizer loaded
weight_dict got
[30, 32, 33, 34]
tensor([27.5072, 27.5778, 47.4172, 21.0830], device='cuda:0')
[30, 31, 32, 33, 34, 35, 36, 37, 38, 39]
data loaded
momentum_net loaded
moco_model loaded
optimizer loaded
weight_dict got
[41, 47, 48, 49]
tensor([18.8273, 16.0583, 15.5594, 24.6708], device='cuda:0')
[40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
data loaded
momentum_net loaded
moco_model loaded
optimizer loaded
weight_dict got
[50, 51, 52, 53, 55, 57, 58, 59]
tensor([87.0146, 19.4770, 22.5394, 53.6799, 16.2714, 26.7794, 19.1453, 20.9934],
       device='cuda:0')
[50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
